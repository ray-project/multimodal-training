# Condition 4: Sequence Parallelism + Data Parallelism (vision) & Tensor Parallelism + Data Parallelism (language)
# 8 GPUs, parallel_size=4, dp_size=2, local batch=1, global batch=2
debug: false

# Vision model configuration - Sequence Parallelism + Data Parallelism
vision:
  model_type: "qwen2_5_vl"
  model_name: "Qwen/Qwen2.5-VL-7B-Instruct"
  parallelism: "sequence"
  dtype: "bfloat16"
  attention_backend: "flash_attention_2"
  activation_checkpointing: true
  autocast: true
  zero_stage: 1

# Text model configuration - Tensor Parallelism + Data Parallelism
# Using autotp to properly handle TP+DP combination
text:
  model_type: "qwen2_5_vl"
  model_name: "Qwen/Qwen2.5-VL-7B-Instruct"
  parallelism: "autotp"
  dtype: "bfloat16"
  attention_backend: "flash_attention_2"
  activation_checkpointing: true
  autocast: true
  zero_stage: 1
  autotp_size: 4
  tp_overlap_comm: false

# Training configuration
training:
  batch_size: 1  # Local batch size per GPU (global batch = 1 * dp_size = 2)
  learning_rate: 1e-5
  num_epochs: 1
  num_iterations: 50
  warmup_steps: 3
  warmup_ratio: 0.0
  lr_scheduler_type: "constant"
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  seed: 42
  parallel_size: 4  # SP/TP size (number of processes per model)
  dp_size: 2  # Data parallelism size
  collocate: true
  clip_grad_norm: false
  max_grad_norm: 1.0
  no_checkpoint: true
  checkpoint_dir: "/tmp/checkpoints"
  log_interval: 1

# Data configuration
data:
  datasets:
    - mscoco2017_train_captions
  data_registry:
    mscoco2017_train_captions:
      annotation_path: "/mnt/local_storage/mscoco2017/annotations/coco2017_train_qwen.json"
      data_path: "/mnt/local_storage/mscoco2017"
    mscoco2017_val_captions:
      annotation_path: "/mnt/local_storage/mscoco2017/annotations/coco2017_val_qwen.json"
      data_path: "/mnt/local_storage/mscoco2017"
  num_workers: 4
  pin_memory: true
  data_flatten: true
  min_pixels: 784
  max_pixels: 200704
  force_fixed_size: true

# DeepSpeed configuration
deepspeed:
  reduce_bucket_size: 500000000
